# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AtFsEP2wjN4--tgh-U1B5ynbiFM2pNUB
"""

from google.colab import files
from IPython.display import Image, display
uploaded = files.upload()

for filename in uploaded.keys():
    print("Uploaded:", filename)

for filename in uploaded.keys():
    print(f"Displaying: {filename}")
    display(Image(filename=filename, width=600, height=300))

"""**This project creates a complete pipeline for recommending comparable properties in real‐estate appraisals. We start by loading and simplifying a nested dataset that includes a subject property, a pool of candidates, and the actual comparables chosen by appraisers. We clean and convert all numerical information into usable formats, then calculate key differences,such as living area, age, location, lot size, price, and whether the structure type and bedroom count match between each subject and candidate. Using these features, we train a machine learning model to distinguish true comparables from other candidates. At prediction time, the model scores all possible candidates for a new subject and returns the top matches. To make the recommendations transparent, we generate visual explanations showing how each feature influenced the model’s decision. We also offer optional enhancements that group candidates by similarity in their intrinsic characteristics and use additional techniques for refined ranking and local explanation. Overall, the notebook provides a clear, interpretable workflow from raw appraisal data to ranked recommendations with visual insights**"""

!pip install pandas numpy scikit-learn shap joblib

JSON_PATH = "/content/appraisals_dataset.json"

import os

JSON_PATH = "/content/appraisals_dataset.json"


print("Looking for JSON at:", JSON_PATH)
print("Exists? ", os.path.exists(JSON_PATH))

!pip install pandas numpy scikit-learn shap joblib matplotlib

import os, re, json
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score
import shap
import matplotlib.pyplot as plt
from IPython.display import display
import math

JSON_PATH = "/content/appraisals_dataset.json"
print("Looking for JSON at:", JSON_PATH)
if not os.path.exists(JSON_PATH):
    raise FileNotFoundError(f"Cannot find {JSON_PATH}. Please double‐check the path.")

# Load the raw json into directory
with open(JSON_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

# We now build a dataframe for appraisal, subjects, candidates and comps
#one row per appraisal (orderID, effective_date, municipality)

app_rows = []
for appraisal in data.get("appraisals", []):
    app_rows.append({
        "orderID": appraisal.get("orderID"),
        "effective_date": appraisal.get("subject", {}).get("effective_date"),
        "municipality": appraisal.get("subject", {}).get("municipality_district")
    })
df_appraisals = pd.DataFrame(app_rows)

# thesame techniwue i.e one row per subject (address, gla, year_built, num_beds)
subj_rows = []
for appraisal in data.get("appraisals", []):
    subj = appraisal.get("subject", {})
    subj_rows.append({
        "orderID": appraisal.get("orderID"),
        "address": subj.get("address"),
        "gla": subj.get("gla"),
        "year_built": subj.get("year_built"),
        "num_beds": subj.get("num_beds")
    })
df_subjects = pd.DataFrame(subj_rows)
# We have to take account of redundancy, flatten every appraisal’s “properties” list, then drop duplicates by “id”
props_list = []
for appraisal in data.get("appraisals", []):
    oid = appraisal.get("orderID")
    for prop in appraisal.get("properties", []):
        entry = prop.copy()
        entry["orderID"] = oid
        props_list.append(entry)
df_candidates = pd.DataFrame(props_list)
if "id" in df_candidates.columns:
    df_candidates = df_candidates.drop_duplicates(subset="id").reset_index(drop=True)

# finally we flatten every appraisal’s “comps” list
comps_list = []
for appraisal in data.get("appraisals", []):
    oid = appraisal.get("orderID")
    for comp in appraisal.get("comps", []):
        entry = comp.copy()
        entry["orderID"] = oid
        comps_list.append(entry)
df_comps = pd.DataFrame(comps_list)

print(f"Loaded:")
print(f"   {len(df_appraisals)} appraisals")
print(f"   {len(df_subjects)} subjects (one per appraisal)")
print(f"   {len(df_candidates)} unique candidate properties")
print(f"   {len(df_comps)} total comp entries")


def parse_float(x):
    """
    Extract the first numeric occurrence (integer or decimal) from x.
    Returns float or np.nan if no number found.
    Examples:
      parse_float("1500 SqFt") → 1500.0
      parse_float("327,000")   → 327000.0
      parse_float("n/a")       → np.nan
    """
    if pd.isna(x):
        return np.nan
    if isinstance(x, (int, float)):
        return float(x)
    s = str(x)
    s_clean = s.replace(",", "")
    match = re.search(r"(\d+(\.\d+)?)", s_clean)
    return float(match.group(1)) if match else np.nan

# We implement The Haversine distance
# This is basically is the shortest‐path distance between two points on Earth’s surface
# It is computed from their latitudes and longitudes accounting for the planet’s curvature.
def haversine_distance(lat1, lon1, lat2, lon2):
    """
    Returns Haversine distance (in kilometers) between (lat1, lon1) and (lat2, lon2).
    If any coordinate is missing/NaN, return 999.0.
    """
    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):
        return 999.0
    φ1, φ2 = math.radians(lat1), math.radians(lat2)
    Δφ = math.radians(lat2 - lat1)
    Δλ = math.radians(lon2 - lon1)
    a = (math.sin(Δφ/2) ** 2 +
         math.cos(φ1) * math.cos(φ2) * (math.sin(Δλ/2) ** 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    return 6371.0 * c

# Feature Ingeenering
def prepare_features(subject_row, candidate_df):
    """
    Given:
      - subject_row: a pd.Series with keys ['address','gla','year_built','num_beds']
      - candidate_df: DataFrame with candidate columns
        like ['gla', 'year_built', 'latitude', 'longitude', 'lot_size_sf', 'close_price', 'structure_type', 'bedrooms']

    Returns a DataFrame of features:
      ['cand_id', 'gla_diff', 'age_diff', 'distance_km', 'lot_diff', 'price_diff',
       'same_structure_type', 'same_bed_count']
    """
    subj_gla   = parse_float(subject_row["gla"])
    subj_year  = parse_float(subject_row["year_built"])
    subj_beds  = parse_float(subject_row["num_beds"])
    subj_lat, subj_lon = np.nan, np.nan
    subj_lot, subj_price = np.nan, np.nan
    subj_struct = ""

    feats = pd.DataFrame()
    if "id" in candidate_df.columns:
        feats["cand_id"] = candidate_df["id"].astype(int)
    else:
        feats["cand_id"] = np.arange(len(candidate_df), dtype=int)

    # GLA diff
    cand_gla = candidate_df["gla"].apply(parse_float)
    feats["gla_diff"] = np.abs(subj_gla - cand_gla)
    if len(feats["gla_diff"]) > 0:
        feats["gla_diff"] = feats["gla_diff"].fillna(feats["gla_diff"].max() * 1.5)
    else:
        feats["gla_diff"] = 0.0

    # Age diff
    cand_year = candidate_df["year_built"].apply(parse_float)
    cand_age = 2025 - cand_year
    subj_age = 2025 - subj_year
    feats["age_diff"] = np.abs(subj_age - cand_age)
    if len(feats["age_diff"]) > 0:
        feats["age_diff"] = feats["age_diff"].fillna(feats["age_diff"].max() * 1.5)
    else:
        feats["age_diff"] = 0.0

    # Distance km
    feats["distance_km"] = candidate_df.apply(
        lambda r: haversine_distance(
            subj_lat, subj_lon,
            parse_float(r.get("latitude")), parse_float(r.get("longitude"))
        ), axis=1
    )

    # Lot diff
    cand_lot = candidate_df["lot_size_sf"].apply(parse_float)
    feats["lot_diff"] = np.abs(subj_lot - cand_lot)
    if len(feats["lot_diff"]) > 0:
        feats["lot_diff"] = feats["lot_diff"].fillna(feats["lot_diff"].max() * 1.5)
    else:
        feats["lot_diff"] = 0.0

    # Price diff
    cand_price = candidate_df["close_price"].apply(parse_float)
    feats["price_diff"] = np.abs(subj_price - cand_price)
    if len(feats["price_diff"]) > 0:
        feats["price_diff"] = feats["price_diff"].fillna(feats["price_diff"].max() * 1.5)
    else:
        feats["price_diff"] = 0.0

    # same_structure_type?
    if "structure_type" in candidate_df.columns:
        feats["same_structure_type"] = (candidate_df["structure_type"] == subj_struct).astype(int)
    else:
        feats["same_structure_type"] = 0

    # same_bed_count?
    if "bedrooms" in candidate_df.columns:
        feats["same_bed_count"] = (candidate_df["bedrooms"].apply(parse_float) == subj_beds).astype(int)
    else:
        feats["same_bed_count"] = 0

    return feats

# Training set
all_feature_rows = []
all_labels = []

for _, subj in df_subjects.iterrows():
    oid = subj["orderID"]
    candidates = df_candidates.copy()

    # Build matching key: lower(address) + "|" + sale_price(no commas)
    candidates["key"] = (
        candidates["address"].fillna("")
            .str.replace(r"\s+", " ", regex=True)
            .str.strip()
            .str.lower()
        + "|"
        + candidates["close_price"].apply(lambda x: str(x).replace(",", ""))
    )

    true_comps = df_comps[df_comps["orderID"] == oid].copy()
    true_comps["key"] = (
        true_comps["address"].fillna("")
            .str.replace(r"\s+", " ", regex=True)
            .str.strip()
            .str.lower()
        + "|"
        + true_comps["sale_price"].str.replace(",", "")
    )

    candidates["is_comp"] = candidates["key"].isin(true_comps["key"]).astype(int)
    candidates.drop(columns=["key"], inplace=True)

    feats = prepare_features(subj, candidates)
    all_feature_rows.append(feats)
    all_labels.append(candidates["is_comp"])

X_all = pd.concat(all_feature_rows, ignore_index=True)
y_all = pd.concat(all_labels, ignore_index=True)
print(f"\nBuilt training set: {X_all.shape[0]} rows × {X_all.shape[1]} features")

# We Train a Random Forect
# Basically averaging bunch of decision trees
# Another option will be a Grid seach
# Overfitting and Under-fitting are commom issues here. We can tune those parameters in a future case.
X_train, X_val, y_train, y_val = train_test_split(
    X_all.drop(columns=["cand_id"]), y_all,
    test_size=0.20, random_state=42, stratify=y_all
)
clf = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)
clf.fit(X_train, y_train)

# Evaluate
if len(clf.classes_) == 1:
    # This will be a dummy
    # Only one class was present in training (all 0s or all 1s)
    # predict_proba will return shape (n_samples, 1).
    # We define “probability of class 1” as all zeros.
    precision, recall = 0.0, 0.0
else:
    y_pred = clf.predict(X_val)
    precision = precision_score(y_val, y_pred, zero_division=0)
    recall = recall_score(y_val, y_pred, zero_division=0)

print(f"\nValidation Precision: {precision:.3f} | Recall: {recall:.3f}")

sample_idx = 0
subject_row = df_subjects.iloc[sample_idx]
print(f"\nSample subject (orderID={subject_row['orderID']}, address={subject_row['address']})")

feats_demo = prepare_features(subject_row, df_candidates)
X_demo = feats_demo.drop(columns=["cand_id"])

# If the model has only one class, assign all zeros; else use predict_proba
if len(clf.classes_) == 1:
    probs_demo = np.zeros(len(X_demo))
else:
    probs_demo = clf.predict_proba(X_demo)[:, 1]

df_candidates_scored = df_candidates.reset_index(drop=True).copy()
df_candidates_scored["score"] = probs_demo
top3 = df_candidates_scored.sort_values("score", ascending=False).head(3).reset_index(drop=True)

print("\nTop 3 recommended comps:")
display(top3[["id", "address", "gla", "bedrooms", "close_price", "latitude", "longitude", "score"]])

# Explain top 3
feats_top3 = prepare_features(subject_row, top3)
X_top3 = feats_top3.drop(columns=["cand_id"])
explainer = shap.TreeExplainer(clf)
if len(clf.classes_) == 1:
    # If only one class, shap_values will be a single array with shape (n_samples, n_features)
    shap_values = [None, explainer.shap_values(X_top3)]
else:
    shap_values = explainer.shap_values(X_top3)

print("\nGenerating SHAP explanations for the top 3 …")
for i, row in top3.iterrows():
    print(f"\n--- Explanation for Candidate ID {row['id']} (Score = {row['score']:.3f}) ---")
    plt.figure(figsize=(6, 4))
    if len(clf.classes_) == 1:
        # Only one class => shap_values[1] is actually the same as shap_values[0]
        shap.force_plot(
            explainer.expected_value,
            shap_values[1][i],
            X_top3.iloc[i],
            matplotlib=True,
            show=True
        )
    else:
        shap.force_plot(
            explainer.expected_value[1],
            shap_values[1][i],
            X_top3.iloc[i],
            matplotlib=True,
            show=True
        )
    plt.tight_layout()
    plt.show()

!pip install lime

# Similarity Search
# Clustering
# Rankung
# Placeholder

# Import the libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt


try:
    import lime
    import lime.lime_tabular
    lime_available = True
except ModuleNotFoundError:
    print("LIME is not installed in this environment. Skipping LIME explainability.")
    lime_available = False

# We will extract candidate-level numeric features .
candidates = df_candidates.copy()

# Parse numeric fields (using the previously defined parse_float)
candidates["gla_num"] = candidates["gla"].apply(parse_float)
candidates["year_built_num"] = candidates["year_built"].apply(parse_float)
candidates["lot_size_num"] = candidates["lot_size_sf"].apply(parse_float)
candidates["price_num"] = candidates["close_price"].apply(parse_float)
candidates["lat_num"] = candidates["latitude"].apply(parse_float)
candidates["lon_num"] = candidates["longitude"].apply(parse_float)
candidates["beds_num"] = candidates["bedrooms"].apply(parse_float)

# We can fill the nan/missing values with standardize score
# We could also fill them with K-nearest mean neighborhood(Many ways to cross the ocean)
# Fill missing numeric values with column means
for col in ["gla_num", "year_built_num", "lot_size_num", "price_num", "lat_num", "lon_num", "beds_num"]:
    candidates[col].fillna(candidates[col].mean(), inplace=True)

# Intrinsic feature columns
intrinsic_features = ["gla_num", "year_built_num", "lot_size_num", "price_num", "lat_num", "lon_num", "beds_num"]
X_intrinsic = candidates[intrinsic_features].values

# Standardize for clustering/similarity
scaler = StandardScaler()
X_intrinsic_norm = scaler.fit_transform(X_intrinsic)

# Choose 10 clusters
num_clusters = 10
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
candidates["cluster"] = kmeans.fit_predict(X_intrinsic_norm)

# Check how many candidates per cluster
cluster_counts = candidates["cluster"].value_counts().sort_index()
print("Candidate counts per cluster:")
print(cluster_counts.to_frame(name="count"))

from sklearn.neighbors import NearestNeighbors

# Nearest Neighbor candidate
# Build a NearestNeighbors index on the normalized intrinsic features
nn_model = NearestNeighbors(n_neighbors=20, algorithm='ball_tree').fit(X_intrinsic_norm)

# Function to build a subject‐intrinsic feature vector (same length/order as intrinsic_features)
def build_subject_intrinsic(subj_row):
    """
    Create a feature vector [gla_num, year_built_num, lot_size_num, price_num, lat_num, lon_num, beds_num]
    for a subject. Missing values are replaced by the mean of candidate columns used above.
    """
    subj_gla = parse_float(subj_row["gla"])
    subj_year = parse_float(subj_row["year_built"])
    subj_lot = np.nan
    subj_price = np.nan
    subj_lat = np.nan
    subj_lon = np.nan
    subj_beds = parse_float(subj_row["num_beds"])

    # Replace missing/nan with candidate‐column means
    subj_vals = [
        subj_gla if not np.isnan(subj_gla) else candidates["gla_num"].mean(),
        subj_year if not np.isnan(subj_year) else candidates["year_built_num"].mean(),
        subj_lot if not np.isnan(subj_lot) else candidates["lot_size_num"].mean(),
        subj_price if not np.isnan(subj_price) else candidates["price_num"].mean(),
        subj_lat if not np.isnan(subj_lat) else candidates["lat_num"].mean(),
        subj_lon if not np.isnan(subj_lon) else candidates["lon_num"].mean(),
        subj_beds if not np.isnan(subj_beds) else candidates["beds_num"].mean()
    ]
    # Normalize using the same scaler
    subj_intrinsic_norm = scaler.transform([subj_vals])
    return subj_intrinsic_norm

sample_idx = 0
subject_row = df_subjects.iloc[sample_idx]
subject_intr_norm = build_subject_intrinsic(subject_row)

distances, indices = nn_model.kneighbors(subject_intr_norm)
neighbor_inds = indices[0]
neighbor_dists = distances[0]

print("\n20 nearest neighbors (by intrinsic distance) to subject:")
print(neighbor_inds[:5], "... distances:", neighbor_dists[:5])

# Find which cluster the subject belongs to
subj_cluster = kmeans.predict(subject_intr_norm)[0]
print(f"\nSubject belongs to cluster: {subj_cluster}")

# Filter candidates to that cluster
cluster_candidates = candidates[candidates["cluster"] == subj_cluster].reset_index(drop=True)
print(f"Candidates in same cluster: {len(cluster_candidates)}")

# Compute "comparable" features for cluster candidates vs. the subject
feats_cluster = prepare_features(subject_row, cluster_candidates)
X_cluster = feats_cluster.drop(columns=["cand_id"])

# Score by our trained classifier (clf) if it has two classes, else zeros
if len(clf.classes_) == 1:
    probs_cluster = np.zeros(len(X_cluster))
else:
    probs_cluster = clf.predict_proba(X_cluster)[:, 1]

cluster_candidates = cluster_candidates.copy()
cluster_candidates["prob_score"] = probs_cluster

# Compute intrinsic Euclidean distance to subject
subj_intr = subject_intr_norm.flatten()
cand_intr_norm = scaler.transform(cluster_candidates[intrinsic_features])
intrinsic_distances = np.linalg.norm(cand_intr_norm - subj_intr, axis=1)
cluster_candidates["intrinsic_dist"] = intrinsic_distances

# Want high classifier score and low intrinsic distance.
# combined_score = prob_score - 0.1 * intrinsic_dist
cluster_candidates["combined_score"] = (
    cluster_candidates["prob_score"]
    - 0.1 * cluster_candidates["intrinsic_dist"]
)

# Show top 5 by combined_score
top5_ranked = cluster_candidates.sort_values("combined_score", ascending=False).head(5)
print("\nTop 5 ranked comps within the same cluster:")
display(top5_ranked[["id", "address", "prob_score", "intrinsic_dist", "combined_score"]])

# Lime Explainability

if lime_available and (len(top5_ranked) > 0):
    from sklearn.model_selection import train_test_split

    # Prepare a training subset (same features used above)
    # We reuse X_train from earlier
    # For safety, we check existence
    try:
        X_train_data = X_train
    except NameError:
        # Rebuild X_train, y_train from X_all, y_all
        X_train_data, X_val_data, y_train_data, y_val_data = train_test_split(
            X_all.drop(columns=["cand_id"]), y_all,
            test_size=0.20, random_state=42, stratify=y_all
        )
        X_train_data = X_train_data

    feature_names = [
        "gla_diff",
        "age_diff",
        "distance_km",
        "lot_diff",
        "price_diff",
        "same_structure_type",
        "same_bed_count"
    ]

    explainer_lime = lime.lime_tabular.LimeTabularExplainer(
        training_data=X_train_data.values,
        feature_names=feature_names,
        class_names=["not_comp", "comp"],
        discretize_continuous=True
    )

    # Pick the top‐ranked candidate
    candidate_example = top5_ranked.iloc[0]
    feat_example = prepare_features(subject_row, pd.DataFrame([candidate_example]))
    X_ex = feat_example.drop(columns=["cand_id"]).values

    if len(clf.classes_) == 1:
        print("\nLIME skipped: classifier has only one class.")
    else:
        exp = explainer_lime.explain_instance(
            data_row=X_ex[0],
            predict_fn=clf.predict_proba,
            num_features=5
        )
        print(f"\nLIME explanation for Candidate ID {candidate_example['id']}:")
        exp.show_in_notebook(show_table=True)
else:
    print("\nLIME explainability skipped (either LIME not installed or no candidates).")





import matplotlib.pyplot as plt

cluster_counts = candidates["cluster"].value_counts().sort_index()

plt.figure(figsize=(8, 4))
plt.bar(cluster_counts.index.astype(str), cluster_counts.values, color="skyblue", edgecolor="black")
plt.xlabel("Cluster Label")
plt.ylabel("Number of Candidates")
plt.title("Number of Candidates per K-Means Cluster")
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reduce high-dim intrinsic features to 2D for plotting
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_intrinsic_norm)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    X_pca[:, 0],
    X_pca[:, 1],
    c=candidates["cluster"],
    cmap="tab10",
    alpha=0.6,
    s=15
)
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("2D PCA of Candidate Intrinsic Features (Colored by Cluster)")
plt.show()

import matplotlib.pyplot as plt

# Parse subject latitude/longitude (if available)
subj_lat = parse_float(subject_row.get("latitude", np.nan))
subj_lon = parse_float(subject_row.get("longitude", np.nan))

# Top-5 ranked candidates’ lat/lon
top5_lats = top5_ranked["latitude"].apply(parse_float)
top5_lons = top5_ranked["longitude"].apply(parse_float)

plt.figure(figsize=(6, 6))

# Plot all cluster candidates in light gray
cluster_lats = cluster_candidates["latitude"].apply(parse_float)
cluster_lons = cluster_candidates["longitude"].apply(parse_float)
plt.scatter(cluster_lons, cluster_lats, color="lightgray", s=10, label="Cluster Candidates")

# Plot top 5 in red triangles
plt.scatter(top5_lons, top5_lats, color="red", s=50, marker="^", label="Top 5 Ranked")

# Plot subject location as a blue star (if lat/lon exist)
if not np.isnan(subj_lat) and not np.isnan(subj_lon):
    plt.scatter([subj_lon], [subj_lat], color="blue", s=80, marker="*", label="Subject Property")

plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Subject Property and Top 5 Ranked Candidates by Location")
plt.legend()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 4))
plt.hist(cluster_candidates["combined_score"], bins=20, color="skyblue", edgecolor="black")
plt.xlabel("Combined Score (prob_score - 0.1 * intrinsic_dist)")
plt.ylabel("Frequency")
plt.title("Distribution of Combined Scores in Subject’s Cluster")
plt.show()

